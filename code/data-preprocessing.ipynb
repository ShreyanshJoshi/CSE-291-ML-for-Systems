{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pymysql\n",
    "import sqlite3\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import signal\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hemilp/CSE_291_Virt/CSE-291-ML-for-Systems/code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_dbs = ['car_1', 'wine_1', 'student_1', 'inn_1', 'flight_1', 'formula_1', 'restaurants']\n",
    "# sqlite_dbs = ['car_1', 'wine_1', 'student_1', 'inn_1', 'flight_1', 'formula_1']\n",
    "mysql_dbs = ['advising', 'atis', 'geography', 'imdb']\n",
    "\n",
    "independent_dbs = ['advising', 'atis', 'geography', 'restaurants', 'imdb']\n",
    "spider_dbs = set(['car_1', 'wine_1', 'student_1', 'inn_1', 'flight_1', 'formula_1'])\n",
    "\n",
    "json_data_dir = '../data/json/'\n",
    "csv_data_dir = '../data/csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries(db):\n",
    "    # spider.json contains query for 6 different dbs\n",
    "    spider = False\n",
    "    if db == \"spider\":\n",
    "        spider = True\n",
    "        \n",
    "    queryFile = os.path.join(json_data_dir, f'{db}.json')\n",
    "\n",
    "    print(queryFile)\n",
    "                             \n",
    "    # Open the JSON file and load its contents\n",
    "    with open(queryFile, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    query_dict = defaultdict(list)\n",
    "    for d in data:\n",
    "        variables = d['variables']\n",
    "        sql_queries = d['sql']\n",
    "\n",
    "        if spider and d[\"sentences\"][0][\"database\"] not in spider_dbs:\n",
    "            continue\n",
    "\n",
    "        # replacing each occurrence of a particular variable in any of the queries\n",
    "        for sql_query in sql_queries:\n",
    "            query = sql_query\n",
    "            for var in variables:\n",
    "                query = query.replace(var['name'], var['example'])\n",
    "                \n",
    "            if spider:\n",
    "                query = re.sub(r'\\( .+\\.\\* \\)', '( * )', query)\n",
    "                query_dict[d[\"sentences\"][0][\"database\"]].append(query)\n",
    "            else:\n",
    "                query_dict[db].append(query)\n",
    "    \n",
    "    return query_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/json/spider.json\n",
      "car_1 45\n",
      "wine_1 41\n",
      "student_1 27\n",
      "formula_1 40\n",
      "inn_1 36\n",
      "flight_1 48\n"
     ]
    }
   ],
   "source": [
    "# Get queries for each database in spider\n",
    "spider_query_dict = get_queries('spider')\n",
    "for db in spider_dbs:\n",
    "    query_list = spider_query_dict[db]\n",
    "    with open(os.path.join(csv_data_dir, f'{db}.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['query'])\n",
    "        print(db, len(query_list))\n",
    "        for query in query_list:\n",
    "            writer.writerow([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/json/advising.json\n",
      "advising 214\n",
      "../data/json/atis.json\n",
      "atis 1134\n",
      "../data/json/geography.json\n",
      "geography 259\n",
      "../data/json/restaurants.json\n",
      "restaurants 23\n",
      "../data/json/imdb.json\n",
      "imdb 99\n"
     ]
    }
   ],
   "source": [
    "# Get queries for independent dbs\n",
    "for db in independent_dbs:\n",
    "# for db in ['advising']:\n",
    "    query_list = get_queries(db)[db]\n",
    "    with open(os.path.join(csv_data_dir, f'{db}.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['query'])\n",
    "        print(db, len(query_list))\n",
    "        for query in query_list:\n",
    "            writer.writerow([query])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below code blocks were used for data analysis of spider dataset as it contained multiple databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/json/spider.json\n",
      "../data/json/spider.json\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "all_queries = []\n",
    "\n",
    "# all_queries.extend(get_queries(\"./atis.json\"))\n",
    "queries = dict()\n",
    "\n",
    "import os\n",
    "for filename in os.listdir('../data/json'):\n",
    "    if filename.endswith('spider.json'):\n",
    "        # remove the .json extension\n",
    "        filename = filename.split('.')[0]\n",
    "        all_queries.extend(get_queries(filename))\n",
    "        queries[filename] = get_queries(filename)\n",
    "\n",
    "    \n",
    "print(len(all_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_queries)\n",
    "for filename in os.listdir('.'):\n",
    "    if filename.endswith('spider.json'):\n",
    "        print(f'{filename}: {len(queries[filename])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'storm_record', 'college_3', 'gymnast', 'flight_4', 'scholar', 'debate', 'medicine_enzyme_interaction', 'swimming', 'product_catalog', 'customers_and_products_contacts', 'customer_complaints', 'insurance_policies', 'local_govt_and_lot', 'match_season', 'decoration_competition', 'apartment_rentals', 'culture_company', 'small_bank_1', 'game_injury', 'solvency_ii', 'ship_1', 'local_govt_in_alabama', 'network_2', 'e_learning', 'shop_membership', 'railway', 'dorm_1', 'battle_death', 'academic', 'program_share', 'tracking_grants_for_research', 'music_2', 'singer', 'flight_company', 'pilot_record', 'film_rank', 'riding_club', 'tracking_orders', 'car_1', 'orchestra', 'performance_attendance', 'tracking_software_problems', 'cre_Doc_Template_Mgt', 'activity_1', 'journal_committee', 'wedding', 'restaurant_1', 'mountain_photos', 'body_builder', 'allergy_1', 'epinions_1', 'company_employee', 'roller_coaster', 'cre_Drama_Workshop_Groups', 'aircraft', 'news_report', 'ship_mission', 'device', 'entertainment_awards', 'tvshow', 'student_1', 'wta_1', 'company_office', 'student_transcripts_tracking', 'city_record', 'customers_card_transactions', 'tracking_share_transactions', 'manufactory_1', 'gas_company', 'cre_Docs_and_Epenses', 'county_public_safety', 'wrestler', 'department_store', 'party_host', 'assets_maintenance', 'voter_2', 'twitter_1', 'wine_1', 'customers_and_invoices', 'music_4', 'poker_player', 'imdb', 'party_people', 'machine_repair', 'yelp', 'station_weather', 'driving_school', 'movie_1', 'restaurants', 'sakila_1', 'behavior_monitoring', 'flight_2', 'cinema', 'sports_competition', 'architecture', 'flight_1', 'cre_Doc_Control_Systems', 'store_product', 'hr_1', 'geo', 'club_1', 'course_teach', 'icfp_1', 'e_government', 'soccer_2', 'scientist_1', 'baseball_1', 'farm', 'cre_Theme_park', 'phone_1', 'formula_1', 'dog_kennels', 'customer_deliveries', 'csu_1', 'network_1', 'insurance_and_eClaims', 'theme_gallery', 'pets_1', 'manufacturer', 'perpetrator', 'election_representative', 'climbing', 'inn_1', 'protein_institute', 'insurance_fnol', 'document_management', 'museum_visit', 'music_1', 'voter_1', 'college_1', 'coffee_shop', 'department_management', 'book_2', 'store_1', 'local_govt_mdm', 'election', 'employee_hire_evaluation', 'student_assessment', 'company_1', 'concert_singer', 'customers_and_addresses', 'entrepreneur', 'school_bus', 'university_basketball', 'phone_market', 'soccer_1', 'candidate_poll', 'real_estate_properties', 'browser_web', 'school_finance', 'college_2', 'cre_Doc_Tracking_DB', 'school_player', 'products_gen_characteristics', 'bike_1', 'loan_1', 'chinook_1', 'hospital_1', 'game_1', 'world_1', 'musical', 'workshop_paper', 'race_track', 'customers_campaigns_ecommerce', 'products_for_hire', 'train_station'}\n",
      "166\n"
     ]
    }
   ],
   "source": [
    "s = set()\n",
    "with open('../data/json/spider.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    sentences = data[i][\"sentences\"]\n",
    "    for sentence in sentences:\n",
    "        s.add(sentence[\"database\"])\n",
    "\n",
    "print(s)\n",
    "print(len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining execution time\n",
    "\n",
    "At this point, we have all the queries. Now, we write a script to run the queries one by one and store their execution times in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_data_dir = \"../data/runtime\"\n",
    "\n",
    "if not os.path.exists(runtime_data_dir):\n",
    "    os.makedir(runtime_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35864/357185806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msqlite_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'./data/db/{db}.sqlite'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlite_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "# excute sqlite queries\n",
    "with open(os.path.join(runtime_data_dir, f'query-runtime-spider.csv'), 'w', newline='') as outfile:\n",
    "    query_writer = csv.writer(outfile)\n",
    "    \n",
    "    for i, db in enumerate(spider_dbs):\n",
    "        sqlite_file = f'./data/db/{db}.sqlite'\n",
    "\n",
    "        connection = sqlite3.connect(sqlite_file)\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        with open(os.path.join(csv_data_dir, f'{db}.csv'), newline='') as csvfile:\n",
    "            query_reader = csv.reader(csvfile)\n",
    "            rows = list(query_reader)\n",
    "\n",
    "        # Add header for the \"running time\" column\n",
    "        if i == 0:\n",
    "            rows[0].append('runtime (ms)')\n",
    "            rows[0].append('db')\n",
    "            rows[0].append('engine')\n",
    "            query_writer.writerows(rows[0:1])\n",
    "\n",
    "        for j, row in enumerate(rows[1:]):\n",
    "            start_time = time.time()\n",
    "            sql_query = row[0]\n",
    "            try:\n",
    "                cursor.execute(sql_query)\n",
    "            except:\n",
    "                print(f'Line {j+2}: {sql_query}')\n",
    "                continue\n",
    "            cursor.connection.commit()\n",
    "            end_time = time.time()\n",
    "            running_time = (end_time - start_time) * 1000\n",
    "            row.append(running_time)\n",
    "            row.append(db)\n",
    "            row.append('sqlite')\n",
    "            \n",
    "            cursor.execute(\"PRAGMA cache_size=0;\")\n",
    "            cursor.connection.commit()\n",
    "\n",
    "            query_writer.writerow(row)\n",
    "\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mSuccessfully started `mysql` (label: homebrew.mxcl.mysql)\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!brew services start mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 50: SELECT NEIGHBORHOODalias0.NAME FROM BUSINESS AS BUSINESSalias0 , NEIGHBORHOOD AS NEIGHBORHOODalias0 , REVIEW AS REVIEWalias0 , USER AS USERalias0 WHERE NEIGHBORHOODalias0.BUSINESS_ID = BUSINESSalias0.BUSINESS_ID AND REVIEWalias0.BUSINESS_ID = BUSINESSalias0.BUSINESS_ID AND USERalias0.NAME = \"Michelle\" AND USERalias0.USER_ID = REVIEWalias0.USER_ID ;\n",
      "Line 84: SELECT COUNT( DISTINCT ( REVIEWalias0.TEXT ) ) , REVIEWalias0.MONTH FROM BUSINESS AS BUSINESSalias0 , REVIEW AS REVIEWalias0 , USER AS USERalias0 WHERE REVIEWalias0.BUSINESS_ID = BUSINESSalias0.BUSINESS_ID AND USERalias0.NAME = \"Michelle\" GROUP BY REVIEWalias0.MONTH ;\n"
     ]
    }
   ],
   "source": [
    "# To create local mysql instance\n",
    "# brew services start mysql\n",
    "# mysql -u root\n",
    "# DROP DATABASE IF EXISTS {db};\n",
    "# CREATE DATABASE {db};\n",
    "# USE {db};\n",
    "# source /path/to/{db}.sql\n",
    "# \\q\n",
    "# brew services stop mysql\n",
    "# excute mysql queries\n",
    "\n",
    "# Define a function to handle the timeout\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Query execution timed out\")\n",
    "\n",
    "# Set the signal alarm to trigger\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    \n",
    "# with open(os.path.join(runtime_data_dir, f'query-runtime-spider.csv'), 'w', newline='') as outfile:\n",
    "with open(os.path.join(runtime_data_dir, f'query-runtime-yelp.csv'), 'w', newline='') as outfile:\n",
    "    query_writer = csv.writer(outfile)\n",
    "#     for i, db in enumerate(['car_1', 'student_1', 'inn_1', 'formula_1', 'restaurants']):\n",
    "    for i, db in enumerate(['yelp']):\n",
    "        connection = pymysql.connect(\n",
    "            host='localhost',\n",
    "            user='root',\n",
    "            password='',\n",
    "            database=db\n",
    "        )\n",
    "        \n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        cursor.execute(\"SET sql_mode=(SELECT REPLACE(@@sql_mode,'ONLY_FULL_GROUP_BY',''));\")\n",
    "        cursor.connection.commit()\n",
    "                \n",
    "        with open(os.path.join(csv_data_dir, f'{db}.csv'), newline='') as csvfile:\n",
    "            query_reader = csv.reader(csvfile)\n",
    "            row = next(query_reader)\n",
    "            if i == 0:\n",
    "                row.append('runtime (ms)')\n",
    "                row.append('db')\n",
    "                row.append('engine')\n",
    "                query_writer.writerows([row])\n",
    "\n",
    "            for j, row in enumerate(query_reader):\n",
    "                signal.alarm(60)  # Timeout after 60 seconds\n",
    "                \n",
    "                start_time = time.time()\n",
    "                sql_query = row[0]\n",
    "                try:\n",
    "                    cursor.execute(sql_query)\n",
    "                    cursor.connection.commit()\n",
    "                except TimeoutError:\n",
    "                    print(f'Line {j+2}: TimeoutError')\n",
    "                    continue\n",
    "                except:\n",
    "                    print(f'Line {j+2}: {sql_query}')\n",
    "                    continue\n",
    "                finally:\n",
    "                    signal.alarm(0)\n",
    "                    connection.ping(reconnect=True) # attempt to reconnect\n",
    "                \n",
    "                end_time = time.time()\n",
    "                running_time = (end_time - start_time) * 1000\n",
    "                row.append(running_time)\n",
    "                row.append(db)\n",
    "                row.append('mysql')\n",
    "\n",
    "                query_writer.writerow(row)\n",
    "                \n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping `mysql`... (might take a while)\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSuccessfully stopped `mysql` (label: homebrew.mxcl.mysql)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!brew services stop mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation complete\n"
     ]
    }
   ],
   "source": [
    "# Merge all the runtime files\n",
    "dbs = ['advising', 'atis', 'geography', 'restaurants', 'imdb', 'spider']\n",
    "\n",
    "output_file = os.path.join(runtime_data_dir, f'query-runtime-all.csv')\n",
    "\n",
    "# Open the output CSV file in write mode\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Iterate over each input file\n",
    "    for i, db in enumerate(dbs):\n",
    "        input_file = os.path.join(runtime_data_dir, f'query-runtime-{db}.csv')\n",
    "        # Open the input CSV file\n",
    "        with open(input_file, 'r', newline='') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            \n",
    "            header = next(reader)\n",
    "            if i == 0:\n",
    "                writer.writerow(header)\n",
    "                \n",
    "            # Iterate over each row in the input file\n",
    "            for row in reader:\n",
    "                # Write the row to the output CSV file\n",
    "                writer.writerow(row)\n",
    "\n",
    "print(\"Concatenation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                   query  runtime (ms)  \\\n",
      "0     SELECT DISTINCT COURSEalias0.ADVISORY_REQUIREM...      8.092165   \n",
      "1     SELECT DISTINCT COURSEalias0.DEPARTMENT , COUR...      0.772238   \n",
      "2     SELECT DISTINCT COURSEalias0.DEPARTMENT , COUR...      0.530958   \n",
      "3     SELECT COUNT( * ) > 0 FROM COURSE AS COURSEali...     92.988014   \n",
      "4     SELECT DISTINCT COURSEalias0.DEPARTMENT , COUR...      1.093864   \n",
      "...                                                 ...           ...   \n",
      "1910  SELECT MAX( RESULTSalias0.FASTESTLAPSPEED ) , ...      1.104116   \n",
      "1911  SELECT AVG( RESULTSalias0.FASTESTLAPSPEED ) , ...      1.092196   \n",
      "1912  SELECT COUNT( * ) , DRIVERSalias0.DRIVERID , D...     16.340971   \n",
      "1913  SELECT COUNT( * ) , DRIVERSalias0.DRIVERID FRO...     15.832901   \n",
      "1914  SELECT DRIVERSalias0.DRIVERID , DRIVERSalias0....     17.425776   \n",
      "\n",
      "             db  engine  \n",
      "0      advising   mysql  \n",
      "1      advising   mysql  \n",
      "2      advising   mysql  \n",
      "3      advising   mysql  \n",
      "4      advising   mysql  \n",
      "...         ...     ...  \n",
      "1910  formula_1  sqlite  \n",
      "1911  formula_1  sqlite  \n",
      "1912  formula_1  sqlite  \n",
      "1913  formula_1  sqlite  \n",
      "1914  formula_1  sqlite  \n",
      "\n",
      "[1915 rows x 4 columns]>\n",
      "query           False\n",
      "runtime (ms)    False\n",
      "db              False\n",
      "engine          False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(os.path.join(runtime_data_dir, f'query-runtime-all.csv'))\n",
    "print(df.head)\n",
    "print(df.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
